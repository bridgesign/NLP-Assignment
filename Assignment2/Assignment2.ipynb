{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/rohan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import random\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open(\"speeches.txt\",'r')\n",
    "text = fp.read()\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub('SPEECH \\d+', '\\n', text)\n",
    "text = re.sub('â€™', \"'\",text)\n",
    "text = text.replace('...',' ')\n",
    "text = text.replace('..',' ')\n",
    "text = re.sub(\"[^A-Za-z.']\",' ',text)\n",
    "text = text.replace('\\n',' ')\n",
    "text = text.replace('  ',' ')\n",
    "text = text.replace('  ',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenize_list = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sentences(sent_tokenize_list):\n",
    "    sent_list = []\n",
    "    vocab = {\"<s>\",\"</s>\"}\n",
    "    for sent in sent_tokenize_list:\n",
    "        tokens = nltk.word_tokenize(sent[:-1].lower())\n",
    "        temp = []\n",
    "        temp.append(\"<s>\")\n",
    "        for j in tokens:\n",
    "            vocab.add(j)\n",
    "            temp.append(j)\n",
    "        temp.append(\"</s>\")\n",
    "        sent_list.append(temp)\n",
    "    return sent_list, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_list, vocab = gen_sentences(sent_tokenize_list)\n",
    "\n",
    "train, test = train_test_split(sent_list, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_l = len(vocab)\n",
    "vocab_train = set()\n",
    "for sent in train:\n",
    "    for word in sent:\n",
    "        vocab_train.add(word)\n",
    "\n",
    "vocab_tl = len(list(vocab_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the n-grams for the specified n\n",
    "# on the sentences in sent_list\n",
    "# It only enumerates the list of existing n-grams\n",
    "# It returns a ditcionary containing ngrams as keys and their counts\n",
    "def get_n_grams(sent_list, n):\n",
    "    n_grams = {}\n",
    "    for sent in sent_list:\n",
    "        for gram in list(nltk.ngrams(sent, n)):\n",
    "            if gram in n_grams.keys():\n",
    "                n_grams[gram]+=1\n",
    "            else:\n",
    "                n_grams[gram]=1\n",
    "    \n",
    "    return n_grams\n",
    "\n",
    "# Calculates all possible n-grams for given\n",
    "# length of vocabulary. The vocabulary conatins <s> and </s>\n",
    "def calculate_possible_ngram(vocab_l, n):\n",
    "    if n==1:\n",
    "        return vocab_l\n",
    "    if n==2:\n",
    "        return 2*(vocab_l-1)*(vocab_l-2)\n",
    "    return ((vocab_l-2)**(n-2))*((vocab_l-1)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unigrams:\t 5189\n",
      "All possible Unigrams:\t 5189\n"
     ]
    }
   ],
   "source": [
    "n = 1\n",
    "unigrams = get_n_grams(train, n)\n",
    "print(\"Number of Unigrams:\\t\", len(list(unigrams.keys())))\n",
    "print(\"All possible Unigrams:\\t\", calculate_possible_ngram(vocab_tl, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Bigrams:\t 41129\n",
      "All possible Bigrams:\t 53820312\n"
     ]
    }
   ],
   "source": [
    "n = 2\n",
    "bigrams = get_n_grams(train, n)\n",
    "print(\"Number of Bigrams:\\t\", len(list(bigrams.keys())))\n",
    "print(\"All possible Bigrams:\\t\", calculate_possible_ngram(vocab_tl, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Trigrams:\t 81450\n",
      "All possible Trigrams:\t 139609889328\n"
     ]
    }
   ],
   "source": [
    "n = 3\n",
    "trigrams = get_n_grams(train, n)\n",
    "print(\"Number of Trigrams:\\t\", len(list(trigrams.keys())))\n",
    "print(\"All possible Trigrams:\\t\", calculate_possible_ngram(vocab_tl, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Bigrams:\t 99721\n",
      "All possible Quadgrams:\t 724156495944336\n"
     ]
    }
   ],
   "source": [
    "n = 4\n",
    "quadgrams = get_n_grams(train, n)\n",
    "print(\"Number of Bigrams:\\t\", len(list(quadgrams.keys())))\n",
    "print(\"All possible Quadgrams:\\t\", calculate_possible_ngram(vocab_tl, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate probability with smoothing\n",
    "def gen_count(ngram, n):\n",
    "    rev_dict = {}\n",
    "    total = 0\n",
    "    for gram in ngram.keys():\n",
    "        total += ngram[gram]\n",
    "        if ngram[gram] not in rev_dict.keys():\n",
    "            rev_dict[ngram[gram]] = 1\n",
    "        else:\n",
    "            rev_dict[ngram[gram]] += 1\n",
    "    \n",
    "    sor_count = list(rev_dict.keys())\n",
    "    sor_count.sort()\n",
    "    return (ngram, rev_dict, sor_count, total, n)\n",
    "\n",
    "# Gets the probability of specific ngram\n",
    "# Using Kneser-Ney smoothing\n",
    "def get_count(text, probDis_obj):\n",
    "    ngram, rev_dict, sor_count, total, n = probDis_obj\n",
    "    if tuple(text) not in ngram.keys():\n",
    "        return float(rev_dict[1])/total\n",
    "    if ngram[tuple(text)] == sor_count[-1]:\n",
    "        return float(sor_count[-1])\n",
    "    index = sor_count.index(ngram[tuple(text)])\n",
    "    fac = sor_count[index+1]\n",
    "    return float(fac*rev_dict[fac])/rev_dict[ngram[tuple(text)]]\n",
    "\n",
    "\n",
    "# Gets the conditional probabiltiy\n",
    "def get_cond_prob(text, probDis_obj, total):\n",
    "    n = probDis_obj[4]\n",
    "    count = get_count(text, probDis_obj)\n",
    "    acount = 0\n",
    "    for gram in probDis_obj[0].keys():\n",
    "        if tuple(text[:-1]) == gram[:-1]:\n",
    "            acount += get_count(gram, probDis_obj)\n",
    "    if acount==0:\n",
    "        acount = total\n",
    "    return count/acount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the perplexity of a sentence\n",
    "def get_perplexity(sentence, probDis, n):\n",
    "    probDis_obj = probDis[n-1]\n",
    "    perplexity = 1\n",
    "    if n==1:\n",
    "        total = probDis[0][3]\n",
    "    else:\n",
    "        total = probDis[n-2][3]\n",
    "    for gram in nltk.ngrams(sentence, n):\n",
    "        perplexity*= (get_cond_prob(gram, probDis_obj, total))**(-1/len(sentence))\n",
    "    for j in range(n-1):\n",
    "        if j==0:\n",
    "            total = probDis[0][3]\n",
    "        else:\n",
    "            total = probDis[j-1][3]\n",
    "        text = sentence[:j+1]\n",
    "        perplexity*= (get_cond_prob(text, probDis[j], total))**(-1/len(sentence))\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates the normalized probabiltiy distribution\n",
    "# for the specificed ngram\n",
    "def normalized_prob_dis(probDis_obj, text, vocab, total):\n",
    "    nm_prob_dis = []\n",
    "    word_list = []\n",
    "    for word in vocab:\n",
    "        nm_prob_dis.append(get_cond_prob(text+list(word), probDis_obj, total))\n",
    "        word_list.append(word[0])\n",
    "    nm_prob_dis = np.array(nm_prob_dis)\n",
    "    fac = np.sum(nm_prob_dis)\n",
    "    return nm_prob_dis/fac, word_list\n",
    "\n",
    "# Generates the sentences for given model\n",
    "def generate_sent(probDis, n, length):\n",
    "    sentence = ['<s>']\n",
    "    vocab, vocab_l = probDis[0][0].keys(), probDis[0][3] \n",
    "    for j in range(1,n-1):\n",
    "        text = sentence[:j]\n",
    "        nm_prob_dis, word_list = normalized_prob_dis(probDis[j], text, vocab, probDis[j-1][3])\n",
    "        index = np.argmax(np.random.multinomial(vocab_l*100, nm_prob_dis, size=1))\n",
    "        sentence.append(word_list[index])\n",
    "    \n",
    "    if n==1:\n",
    "        total = probDis[0][3]\n",
    "    else:\n",
    "        total = probDis[n-2][3]\n",
    "    while len(sentence)< length+1:\n",
    "        text = sentence[-n+1:]\n",
    "        nm_prob_dis, word_list = normalized_prob_dis(probDis[n-1], text, vocab, total)\n",
    "        index = np.argmax(np.random.multinomial(vocab_l*100, nm_prob_dis, size=1))\n",
    "        sentence.append(word_list[index])\n",
    "    return ' '.join(sentence[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_un = gen_count(unigrams, 1)\n",
    "prob_bi = gen_count(bigrams, 2)\n",
    "prob_tri = gen_count(trigrams, 3)\n",
    "prob_quad = gen_count(quadgrams, 4)\n",
    "\n",
    "probDis = (prob_un, prob_bi, prob_tri, prob_quad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity of Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Perplexity: 400.37763818809566\n",
      "Bigram Perplexity: 84.5394947036964\n",
      "Trigram Perplexity: 311.55710770061154\n",
      "Quadgram Perplexity: 1883.0692291105845\n"
     ]
    }
   ],
   "source": [
    "# Get perplexity of test data\n",
    "\n",
    "len_test = len(test)\n",
    "\n",
    "unigram_perplexity = 0\n",
    "for sent in test:\n",
    "    unigram_perplexity+=get_perplexity(sent, probDis, 1)\n",
    "print(\"Unigram Perplexity:\", float(unigram_perplexity)/len_test)\n",
    "\n",
    "\n",
    "bigram_perplexity = 0\n",
    "for sent in test:\n",
    "    bigram_perplexity+=get_perplexity(sent, probDis, 2)\n",
    "print(\"Bigram Perplexity:\", float(bigram_perplexity)/len_test)\n",
    "\n",
    "\n",
    "trigram_perplexity = 0\n",
    "for sent in test:\n",
    "    trigram_perplexity+=get_perplexity(sent, probDis, 3)\n",
    "print(\"Trigram Perplexity:\", float(trigram_perplexity)/len_test)\n",
    "\n",
    "\n",
    "quadgram_perplexity = 0\n",
    "for sent in test:\n",
    "    quadgram_perplexity+=get_perplexity(sent, probDis, 4)\n",
    "print(\"Quadgram Perplexity:\", float(quadgram_perplexity)/len_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the perplexity for Trigrams and Quadgrams is increasing. This is beacuse perplexity is being calculated on a model only with Kneser-Ney smoothing without Backoff. So whenever a stream of words that the model has not seen or not seen in that order at all causes it to give very high perplexity. The sentences in the speech are sometimes very small or are broken due to pauses taken by the speaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate 5 sentences for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram\n",
      "\t hispanic quite fixing depend means illegally genesco hopefully hospitals dreamers\n",
      "\t killer sustainable smiling yearly agreed high immediate foster bye agrees\n",
      "\t firmly sachs registered forced qualities wanted vaccines comedian looks love\n",
      "\t practice deepest fortunately lg endorsement wrecking abusers grass gdp complaining\n",
      "\t penn locations christian rapidly realize fighting listened details remains skills\n",
      "Bigram\n",
      "\t i 'm going to be the people </s> politicians want\n",
      "\t i 'm going to be the people </s> but they\n",
      "\t i 'm going to be the people </s> deserve it\n",
      "\t i 'm going to be the people </s> cnbc ranked\n",
      "\t i 'm going to be the people </s> lasting deal\n",
      "Trigram\n",
      "\t i 'm a messenger </s> mosque pelley stamps talent cashed\n",
      "\t i 'm a messenger </s> rein yesterday squad act beef\n",
      "\t i 'm a messenger </s> dismissed others platform citibank label\n",
      "\t i 'm a messenger </s> cookies savage beautiful crack save\n",
      "\t i 'm a messenger </s> issues ladies footer based policies\n",
      "Quadgram\n",
      "\t i 'm not going to happen </s> edition pan contribution\n",
      "\t i 'm not going to happen </s> seems incentive toupee\n",
      "\t i 'm not going to happen </s> monstrous comply previous\n",
      "\t i 'm not going to happen </s> flipping sucker difficult\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-09969eb8a9ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Quadgram\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgenerate_sent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobDis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-936123f71584>\u001b[0m in \u001b[0;36mgenerate_sent\u001b[0;34m(probDis, n, length)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mnm_prob_dis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalized_prob_dis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobDis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_l\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnm_prob_dis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-936123f71584>\u001b[0m in \u001b[0;36mnormalized_prob_dis\u001b[0;34m(probDis_obj, text, vocab)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mword_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mnm_prob_dis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_cond_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobDis_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mword_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mnm_prob_dis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnm_prob_dis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-6a7a0b0b369d>\u001b[0m in \u001b[0;36mget_cond_prob\u001b[0;34m(text, probDis_obj)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0macount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mgram\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprobDis_obj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0macount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mget_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobDis_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0macount\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generating 5 sentences for each model\n",
    "print(\"Unigram\")\n",
    "for j in range(5):\n",
    "    print('\\t',generate_sent(probDis, 1, 10))\n",
    "print(\"Bigram\")\n",
    "for j in range(5):\n",
    "    print('\\t',generate_sent(probDis, 2, 10))\n",
    "print(\"Trigram\")\n",
    "for j in range(5):\n",
    "    print('\\t',generate_sent(probDis, 3, 10))\n",
    "print(\"Quadgram\")\n",
    "for j in range(5):\n",
    "    print('\\t',generate_sent(probDis, 4, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = list(vocab)\n",
    "word_index = {}\n",
    "\n",
    "for j in range(vocab_l):\n",
    "    word_index[vocab_list[j]] = j\n",
    "\n",
    "reverse_dictionary = dict(zip(word_index.values(), word_index.keys()))\n",
    "test_len = len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_int(sent, reverse_dictionary):\n",
    "    result = []\n",
    "    for word in sent:\n",
    "        result.append(reverse_dictionary[word])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.00001\n",
    "training_iters = len(train)*10\n",
    "display_step = 10000\n",
    "n_input = \n",
    "\n",
    "# number of units in RNN/LSTM cell\n",
    "n_hidden = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input, 1])\n",
    "y = tf.placeholder(\"float\", [None, vocab_l])\n",
    "\n",
    "# RNN/LSTM output node weights and biases\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, vocab_l]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([vocab_l]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # reshape to [1, n_input]\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "\n",
    "    # Generate a n_input-element sequence of inputs\n",
    "    # (eg. [had] [a] [general] -> [20] [6] [33])\n",
    "    x = tf.split(x,n_input,1)\n",
    "\n",
    "    rnn_cell = rnn.BasicRNNCell(n_hidden, reuse =tf.AUTO_REUSE)\n",
    "\n",
    "    # generate prediction\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # there are n_input outputs but\n",
    "    # we only want the last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM(x, weights, biases):\n",
    "\n",
    "    # reshape to [1, n_input]\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "\n",
    "    # Generate a n_input-element sequence of inputs\n",
    "    # (eg. [had] [a] [general] -> [20] [6] [33])\n",
    "    x = tf.split(x,n_input,1)\n",
    "\n",
    "    rnn_cell = rnn.LSTMCell(n_hidden, reuse =tf.AUTO_REUSE)\n",
    "\n",
    "    # generate prediction\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # there are n_input outputs but\n",
    "    # we only want the last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-65773ed4105f>:10: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-14-65773ed4105f>:13: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /media/rohan/Talnor/NLP/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /media/rohan/Talnor/NLP/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py:459: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7fbc61492fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7fbc61492fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7fbc61492fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7fbc61492fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7fbc61492fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7fbc61492fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7fbc61492fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7fbc61492fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From <ipython-input-16-38794a8cfe93>:4: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From /media/rohan/Talnor/NLP/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "pred = RNN(x, weights, biases)\n",
    "\n",
    "# Loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Model evaluation\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter= 10000, Average Loss= 30.637838, Average Accuracy= 0.02%\n",
      "['chuck', 'report'] - [it] vs [creating]\n",
      "Iter= 20000, Average Loss= 19.105214, Average Accuracy= 0.04%\n",
      "['not', 'nice'] - [and] vs [creating]\n",
      "Iter= 30000, Average Loss= 11.909255, Average Accuracy= 3.26%\n",
      "['our', 'veto'] - [which] vs [i]\n",
      "Iter= 40000, Average Loss= 8.690023, Average Accuracy= 6.50%\n",
      "[\"n't\", 'want'] - [to] vs [</s>]\n",
      "Iter= 50000, Average Loss= 7.802065, Average Accuracy= 8.73%\n",
      "['<s>', 'i'] - [was] vs [</s>]\n",
      "Iter= 60000, Average Loss= 7.618074, Average Accuracy= 8.84%\n",
      "['going', 'to'] - [say] vs [</s>]\n",
      "Iter= 70000, Average Loss= 7.547363, Average Accuracy= 8.60%\n",
      "['said', 'well'] - [maybe] vs [</s>]\n",
      "Iter= 80000, Average Loss= 7.465116, Average Accuracy= 8.64%\n",
      "['to', 'stop'] - [just] vs [</s>]\n",
      "Iter= 90000, Average Loss= 7.558892, Average Accuracy= 9.06%\n",
      "['are', 'you'] - [sure] vs [</s>]\n",
      "Iter= 100000, Average Loss= 7.579585, Average Accuracy= 8.73%\n",
      "['were', 'bent'] - [in] vs [</s>]\n",
      "Iter= 110000, Average Loss= 7.568902, Average Accuracy= 8.87%\n",
      "['easiest', 'thing'] - [corey] vs [</s>]\n",
      "Iter= 120000, Average Loss= 7.532544, Average Accuracy= 8.52%\n",
      "[\"n't\", 'think'] - [they] vs [</s>]\n",
      "Iter= 130000, Average Loss= 7.577184, Average Accuracy= 9.10%\n",
      "['i', 'feel'] - [very] vs [</s>]\n",
      "Iter= 140000, Average Loss= 7.534041, Average Accuracy= 9.00%\n",
      "['<s>', 'every'] - [guy] vs [</s>]\n",
      "Optimization Finished!\n",
      "Test Data Perplexity: 1405.9804349257497\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "2 words:  let us\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "let us </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "2 words:  this is\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "2 words:  not working\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not working </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "2 words:  </s> <s>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s> <s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "2 words:  i have\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i have </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    step = 0\n",
    "    offset = random.randint(0,n_input+1)\n",
    "    end_offset = n_input + 1\n",
    "    acc_total = 0\n",
    "    loss_total = 0\n",
    "    \n",
    "    \n",
    "    while step < training_iters:\n",
    "        # Random batch\n",
    "        np.random.shuffle(train)\n",
    "        for sent in train:\n",
    "            offset = 0\n",
    "            while offset <= (len(sent)-end_offset):\n",
    "                symbols_in_keys = [ [word_index[ str(sent[i])]] for i in range(offset, offset+n_input) ]\n",
    "                symbols_in_keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n",
    "\n",
    "                symbols_out_onehot = np.zeros([vocab_l], dtype=float)\n",
    "                symbols_out_onehot[word_index[str(sent[offset+n_input])]] = 1.0\n",
    "                symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1])\n",
    "\n",
    "                _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, pred], \\\n",
    "                                                feed_dict={x: symbols_in_keys, y: symbols_out_onehot})\n",
    "        \n",
    "                loss_total += loss\n",
    "                acc_total += acc\n",
    "                if (step+1) % display_step == 0:\n",
    "                    print(\"Iter= \" + str(step+1) + \", Average Loss= \" + \\\n",
    "                          \"{:.6f}\".format(loss_total/display_step) + \", Average Accuracy= \" + \\\n",
    "                          \"{:.2f}%\".format(100*acc_total/display_step))\n",
    "                    acc_total = 0\n",
    "                    loss_total = 0\n",
    "                    symbols_in = [sent[i] for i in range(offset, offset + n_input)]\n",
    "                    symbols_out = sent[offset + n_input]\n",
    "                    symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, 1).eval())]\n",
    "                    print(\"%s - [%s] vs [%s]\" % (symbols_in,symbols_out,symbols_out_pred))\n",
    "                step += 1\n",
    "                offset += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    \n",
    "    # Calculating the perplexity of test data\n",
    "    offset = 0\n",
    "    perplexity = 0\n",
    "    \n",
    "    for sent in test:\n",
    "        total_loss = 0\n",
    "        offset = 0\n",
    "        while offset <= (len(sent)-end_offset):\n",
    "\n",
    "            symbols_in_keys = [ [word_index[ str(sent[i])]] for i in range(offset, offset+n_input) ]\n",
    "            symbols_in_keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n",
    "\n",
    "            symbols_out_onehot = np.zeros([vocab_l], dtype=float)\n",
    "            symbols_out_onehot[word_index[str(sent[offset+n_input])]] = 1.0\n",
    "            symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1])\n",
    "\n",
    "            loss, onehot_pred = session.run([cost, pred], feed_dict={x: symbols_in_keys, y: symbols_out_onehot})\n",
    "        \n",
    "            offset += 1\n",
    "            total_loss+=loss\n",
    "        perplexity += np.exp(total_loss/(offset+1))/test_len\n",
    "        \n",
    "    print(\"Test Data Perplexity:\", float(perplexity))\n",
    "    \n",
    "    \n",
    "    j = 0\n",
    "    \n",
    "    while j < 5:\n",
    "        prompt = \"%s words: \" % n_input\n",
    "        sentence = input(prompt)\n",
    "        sentence = sentence.strip()\n",
    "        words = sentence.split(' ')\n",
    "        if len(words) != n_input:\n",
    "            continue\n",
    "        try:\n",
    "            symbols_in_keys = [word_index[str(words[i])] for i in range(len(words))]\n",
    "            for i in range(32):\n",
    "                keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n",
    "                onehot_pred = session.run(pred, feed_dict={x: keys})\n",
    "                onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())\n",
    "                sentence = \"%s %s\" % (sentence,reverse_dictionary[onehot_pred_index])\n",
    "                symbols_in_keys = symbols_in_keys[1:]\n",
    "                symbols_in_keys.append(onehot_pred_index)\n",
    "            print(sentence)\n",
    "            j+=1\n",
    "        except:\n",
    "            print(\"Word not in dictionary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input, 1])\n",
    "y = tf.placeholder(\"float\", [None, vocab_l])\n",
    "\n",
    "# RNN/LSTM output node weights and biases\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, vocab_l]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([vocab_l]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-15-7de10fca1cae>:10: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fbc61297da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fbc61297da0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fbc61297da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fbc61297da0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fbc61297da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fbc61297da0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fbc61297da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fbc61297da0>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    }
   ],
   "source": [
    "pred = LSTM(x, weights, biases)\n",
    "\n",
    "# Loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Model evaluation\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter= 10000, Average Loss= 17.496814, Average Accuracy= 0.00%\n",
      "['love', 'you'] - [all] vs [protection]\n",
      "Iter= 20000, Average Loss= 13.319770, Average Accuracy= 0.71%\n",
      "['going', 'to'] - [be] vs [they]\n",
      "Iter= 30000, Average Loss= 10.945726, Average Accuracy= 0.49%\n",
      "['to', 'i'] - [wish] vs [combat]\n",
      "Iter= 40000, Average Loss= 9.344861, Average Accuracy= 6.22%\n",
      "['going', 'to'] - [see] vs [</s>]\n",
      "Iter= 50000, Average Loss= 8.245837, Average Accuracy= 8.88%\n",
      "['our', 'movement'] - [and] vs [</s>]\n",
      "Iter= 60000, Average Loss= 7.750816, Average Accuracy= 9.08%\n",
      "['build', 'a'] - [wall] vs [</s>]\n",
      "Iter= 70000, Average Loss= 7.388376, Average Accuracy= 8.13%\n",
      "['you', 'end'] - [the] vs [</s>]\n",
      "Iter= 80000, Average Loss= 7.175589, Average Accuracy= 7.82%\n",
      "['watching', 'this'] - [for] vs [</s>]\n",
      "Iter= 90000, Average Loss= 7.017310, Average Accuracy= 7.14%\n",
      "['i', \"'m\"] - [not] vs [</s>]\n",
      "Iter= 100000, Average Loss= 7.015175, Average Accuracy= 7.41%\n",
      "['the', 'press'] - [would] vs [the]\n",
      "Iter= 110000, Average Loss= 7.015464, Average Accuracy= 7.29%\n",
      "['be', 'braggadocious'] - [</s>] vs [i]\n",
      "Iter= 120000, Average Loss= 7.085434, Average Accuracy= 6.65%\n",
      "['beautiful', 'statements'] - [about] vs [the]\n",
      "Iter= 130000, Average Loss= 7.049776, Average Accuracy= 7.29%\n",
      "['the', 'press'] - [they] vs [</s>]\n",
      "Optimization Finished!\n",
      "Test Data Perplexity: 692.041261260144\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "2 words:  i 'm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 'm </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "2 words:  okay thi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word not in dictionary\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "2 words:  okay this\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "okay this </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "2 words:  is not\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is not </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "2 words:  working again\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working again </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "2 words:  what must\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what must </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    step = 0\n",
    "    offset = random.randint(0,n_input+1)\n",
    "    end_offset = n_input + 1\n",
    "    acc_total = 0\n",
    "    loss_total = 0\n",
    "    \n",
    "    \n",
    "    while step < training_iters:\n",
    "        # Random batch\n",
    "        np.random.shuffle(train)\n",
    "        for sent in train:\n",
    "            offset = 0\n",
    "            while offset <= (len(sent)-end_offset):\n",
    "                symbols_in_keys = [ [word_index[ str(sent[i])]] for i in range(offset, offset+n_input) ]\n",
    "                symbols_in_keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n",
    "\n",
    "                symbols_out_onehot = np.zeros([vocab_l], dtype=float)\n",
    "                symbols_out_onehot[word_index[str(sent[offset+n_input])]] = 1.0\n",
    "                symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1])\n",
    "\n",
    "                _, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, pred], \\\n",
    "                                                feed_dict={x: symbols_in_keys, y: symbols_out_onehot})\n",
    "        \n",
    "                loss_total += loss\n",
    "                acc_total += acc\n",
    "                if (step+1) % display_step == 0:\n",
    "                    print(\"Iter= \" + str(step+1) + \", Average Loss= \" + \\\n",
    "                          \"{:.6f}\".format(loss_total/display_step) + \", Average Accuracy= \" + \\\n",
    "                          \"{:.2f}%\".format(100*acc_total/display_step))\n",
    "                    acc_total = 0\n",
    "                    loss_total = 0\n",
    "                    symbols_in = [sent[i] for i in range(offset, offset + n_input)]\n",
    "                    symbols_out = sent[offset + n_input]\n",
    "                    symbols_out_pred = reverse_dictionary[int(tf.argmax(onehot_pred, 1).eval())]\n",
    "                    print(\"%s - [%s] vs [%s]\" % (symbols_in,symbols_out,symbols_out_pred))\n",
    "                step += 1\n",
    "                offset += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    \n",
    "    # Calculating the perplexity of test data\n",
    "    offset = 0\n",
    "    perplexity = 0\n",
    "    \n",
    "    for sent in test:\n",
    "        total_loss = 0\n",
    "        offset = 0\n",
    "        while offset <= (len(sent)-end_offset):\n",
    "\n",
    "            symbols_in_keys = [ [word_index[ str(sent[i])]] for i in range(offset, offset+n_input) ]\n",
    "            symbols_in_keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n",
    "\n",
    "            symbols_out_onehot = np.zeros([vocab_l], dtype=float)\n",
    "            symbols_out_onehot[word_index[str(sent[offset+n_input])]] = 1.0\n",
    "            symbols_out_onehot = np.reshape(symbols_out_onehot,[1,-1])\n",
    "\n",
    "            loss, onehot_pred = session.run([cost, pred], feed_dict={x: symbols_in_keys, y: symbols_out_onehot})\n",
    "        \n",
    "            offset += 1\n",
    "            total_loss+=loss\n",
    "        perplexity += np.exp(total_loss/(offset+1))/test_len\n",
    "        \n",
    "    print(\"Test Data Perplexity:\", float(perplexity))\n",
    "    \n",
    "    \n",
    "    j = 0\n",
    "    \n",
    "    while j < 5:\n",
    "        prompt = \"%s words: \" % n_input\n",
    "        sentence = input(prompt)\n",
    "        sentence = sentence.strip()\n",
    "        words = sentence.split(' ')\n",
    "        if len(words) != n_input:\n",
    "            continue\n",
    "        try:\n",
    "            symbols_in_keys = [word_index[str(words[i])] for i in range(len(words))]\n",
    "            for i in range(32):\n",
    "                keys = np.reshape(np.array(symbols_in_keys), [-1, n_input, 1])\n",
    "                onehot_pred = session.run(pred, feed_dict={x: keys})\n",
    "                onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())\n",
    "                sentence = \"%s %s\" % (sentence,reverse_dictionary[onehot_pred_index])\n",
    "                symbols_in_keys = symbols_in_keys[1:]\n",
    "                symbols_in_keys.append(onehot_pred_index)\n",
    "            print(sentence)\n",
    "            j+=1\n",
    "        except:\n",
    "            print(\"Word not in dictionary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple RNN model is unable to generate anything. This is likely because the sentences are short and so the occurence of \\<s> and \\</s> are high. Even when the experiment is again conducted without these markers, it gives the most occuring word as output with little or no variation. This also seems to be the case with LSTM model. In terms of readability, the classical is performing better because it is atleast predicting a possible output. The reason for this is because 1) The sentences are short. 2) The amount of training might be less 3) The amount of data required for training might be less for the neural approach. As the sentences are small, small n_input is likely to give better output. But this may not be the case if we do training as a whole continous text rather than sentence wise training. This was also performed but the imporvement in result was not significant. But the value of n_input was kept small.\n",
    "\n",
    "Even though the classical is giving some output, many times they have no correlation with each other. In terms of perplexity also the classical is beating the neural approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
